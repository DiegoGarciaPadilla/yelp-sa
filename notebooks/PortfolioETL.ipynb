{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bgg1FRTSW3dc"
   },
   "source": [
    "# ETL\n",
    "\n",
    "---\n",
    "\n",
    "**Author:** Diego Antonio Garc√≠a Padilla\n",
    "\n",
    "**Date:** Oct 29, 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q7NJ0YPeYDGP"
   },
   "source": [
    "## Enviroment setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 359
    },
    "id": "W4fFJZEDYJQe",
    "outputId": "0ffe40a3-bb98-4480-b921-b4df866ba5ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ENVIRONMENT CHECK ===\n",
      "Python: 3.10.12\n",
      "JAVA_HOME: /usr/lib/jvm/java-8-openjdk-arm64/jre\n",
      "SPARK_HOME: /opt/spark\n",
      "Driver Memory: 8g\n",
      "Executor Memory: 4g\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "#@title Setup & Environment Verification\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "print(\"=== ENVIRONMENT CHECK ===\")\n",
    "print(f\"Python: {sys.version.split()[0]}\")\n",
    "print(f\"JAVA_HOME: {os.environ.get('JAVA_HOME')}\")\n",
    "print(f\"SPARK_HOME: {os.environ.get('SPARK_HOME')}\")\n",
    "print(f\"Driver Memory: {os.environ.get('SPARK_DRIVER_MEMORY')}\")\n",
    "print(f\"Executor Memory: {os.environ.get('SPARK_EXECUTOR_MEMORY')}\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 400
    },
    "id": "5bgOQ8cpWmZX",
    "outputId": "53adea74-2017-427c-b585-78f7a992132b"
   },
   "outputs": [],
   "source": [
    "#@title Import Libraries\n",
    "\n",
    "# PySpark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, CountVectorizer\n",
    "\n",
    "# SciKit Learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Financial data\n",
    "import yfinance as yf\n",
    "\n",
    "# Hugging Face\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "# Kaggle\n",
    "import kagglehub\n",
    "\n",
    "# Utilities\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "import requests\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import subprocess\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Ve3NLAzpd2bJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PRE-FLIGHT CHECK ===\n",
      "Java: ‚úÖ Available\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/11/05 16:17:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßπ Cleaned up existing Spark session\n",
      "==================================================\n",
      "\n",
      "‚úÖ Spark 3.5.6 initialized successfully\n",
      "   Master: local[8]\n",
      "   App Name: Yelp_Sentiment_Analysis\n",
      "   Driver Memory: 8GB\n",
      "   Spark UI: http://localhost:4040\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/05 16:17:05 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).\n"
     ]
    }
   ],
   "source": [
    "#@title Start Spark session\n",
    "\n",
    "print(\"=== PRE-FLIGHT CHECK ===\")\n",
    "\n",
    "# Verify Java is available\n",
    "try:\n",
    "    java_version = subprocess.check_output(['java', '-version'], stderr=subprocess.STDOUT)\n",
    "    print(\"Java: ‚úÖ Available\")\n",
    "except Exception as e:\n",
    "    print(f\"Java: ‚ùå Not available - {e}\")\n",
    "\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# üî• STOP any existing Spark sessions first\n",
    "try:\n",
    "    SparkContext.getOrCreate().stop()\n",
    "    print(\"üßπ Cleaned up existing Spark session\")\n",
    "except:\n",
    "    print(\"üÜï No existing session to clean\")\n",
    "\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create fresh Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Yelp_Sentiment_Analysis\") \\\n",
    "    .master(\"local[8]\") \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"4g\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"100\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "    .config(\"spark.kryoserializer.buffer.max\", \"1024m\") \\\n",
    "    .config(\"spark.local.dir\", \"/tmp/spark\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "print(f\"\\n‚úÖ Spark {spark.version} initialized successfully\")\n",
    "print(f\"   Master: {spark.sparkContext.master}\")\n",
    "print(f\"   App Name: {spark.sparkContext.appName}\")\n",
    "print(f\"   Driver Memory: 8GB\")\n",
    "print(f\"   Spark UI: http://localhost:4040\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QTdrxZmA4tbV"
   },
   "source": [
    "## Download data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amazon Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: /root/.cache/kagglehub/datasets/yelp-dataset/yelp-dataset/versions/4\n",
      "üìÅ Content of: /root/.cache/kagglehub/datasets/yelp-dataset/yelp-dataset/versions/4\n",
      "\n",
      "  üìÑ Dataset_User_Agreement.pdf\n",
      "     Size: 0.08 MB\n",
      "  üìÑ yelp_academic_dataset_business.json\n",
      "     Size: 113.36 MB\n",
      "  üìÑ yelp_academic_dataset_checkin.json\n",
      "     Size: 273.67 MB\n",
      "  üìÑ yelp_academic_dataset_review.json\n",
      "     Size: 5094.40 MB\n",
      "  üìÑ yelp_academic_dataset_tip.json\n",
      "     Size: 172.24 MB\n",
      "  üìÑ yelp_academic_dataset_user.json\n",
      "     Size: 3207.52 MB\n",
      "\n",
      "üíæ Total size: 8.65 GB (8861.26 MB)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9291705417"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#@title Download Yelp reviews dataset\n",
    "\n",
    "yelp_path = kagglehub.dataset_download(\"yelp-dataset/yelp-dataset\")\n",
    "\n",
    "print(\"Path to dataset files:\", yelp_path)\n",
    "\n",
    "def explore_dataset(path):\n",
    "    print(f\"üìÅ Content of: {path}\\n\")\n",
    "    total_size = 0\n",
    "    \n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            size = os.path.getsize(file_path)\n",
    "            total_size += size\n",
    "            size_mb = size / (1024 * 1024)\n",
    "            print(f\"  üìÑ {file}\")\n",
    "            print(f\"     Size: {size_mb:.2f} MB\")\n",
    "    \n",
    "    total_gb = total_size / (1024 * 1024 * 1024)\n",
    "    print(f\"\\nüíæ Total size: {total_gb:.2f} GB ({total_size / (1024 * 1024):.2f} MB)\")\n",
    "    \n",
    "    return total_size\n",
    "\n",
    "explore_dataset(yelp_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Explotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Already exists: ../data/raw/yelp_reviews_raw.parquet\n",
      "üîÑ Loaded \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#@title Load Yelp Reviews as Spark dataset\n",
    "\n",
    "# Parquet path\n",
    "parquet_path = \"../data/raw/yelp_reviews_raw.parquet\"\n",
    "\n",
    "if os.path.exists(parquet_path):\n",
    "    print(f\"‚úÖ Already exists: {parquet_path}\")\n",
    "    df_reviews = spark.read.parquet(parquet_path)\n",
    "    print(f\"üîÑ Loaded \\n\")\n",
    "else:\n",
    "    # Load the dataset\n",
    "    reviews_file = os.path.join(yelp_path, \"yelp_academic_dataset_review.json\")\n",
    "\n",
    "    # Read JSON file with Spark\n",
    "    df_reviews = spark.read.json(reviews_file)\n",
    "\n",
    "    # Show schema to understand structure\n",
    "    print(\"üìã Schema of Yelp Reviews:\")\n",
    "    df_reviews.printSchema()\n",
    "\n",
    "    # Basic statistics\n",
    "    print(f\"\\nüìä Total reviews: {df_reviews.count():,}\")\n",
    "\n",
    "    # Show sample data\n",
    "    print(\"\\nüîç Sample reviews:\")\n",
    "    df_reviews.show(5, truncate=50)\n",
    "\n",
    "    # Check stars distribution\n",
    "    print(\"\\n‚≠ê Stars distribution:\")\n",
    "    df_reviews.groupBy('stars').count().orderBy('stars').show()\n",
    "\n",
    "    # Check text lengths\n",
    "\n",
    "    print(\"\\nüìù Text statistics:\")\n",
    "    df_reviews.select(\n",
    "        F.avg(F.length(F.col('text'))).alias('avg_length'),\n",
    "        F.min(F.length(F.col('text'))).alias('min_length'),\n",
    "        F.max(F.length(F.col('text'))).alias('max_length')\n",
    "    ).show()\n",
    "\n",
    "    df_reviews.write.parquet(parquet_path, mode=\"overwrite\")\n",
    "    print(f\"\\nüíæ Parquet saved: {parquet_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Already exists: ../data/filtered/yelp_reviews_sentiment.parquet\n",
      "üîÑ Loaded\n"
     ]
    }
   ],
   "source": [
    "#@title Sample dataset\n",
    "\n",
    "# Parquet path\n",
    "parquet_path = \"../data/filtered/yelp_reviews_sentiment.parquet\"\n",
    "\n",
    "if os.path.exists(parquet_path):\n",
    "    print(f\"‚úÖ Already exists: {parquet_path}\")\n",
    "    df_sentiment = spark.read.parquet(parquet_path)\n",
    "    print(f\"üîÑ Loaded\")\n",
    "else:\n",
    "    #  Sample directly from the original dataframe with stars\n",
    "    # 10% sample = ~700K reviews (still >1GB when processed with text)\n",
    "    df_sample = df_reviews.sample(fraction=0.20, seed=42)\n",
    "\n",
    "    # Create sentiment column\n",
    "    df_sentiment = df_sample.select(\n",
    "        F.col('review_id'),\n",
    "        F.col('text'),\n",
    "        F.col('stars'),\n",
    "        F.col('useful'),\n",
    "        F.col('date')\n",
    "    ).withColumn('sentiment',\n",
    "        F.when(F.col('stars').isin([1.0, 2.0]), 'negative')\n",
    "        .when(F.col('stars') == 3.0, 'neutral')\n",
    "        .when(F.col('stars').isin([4.0, 5.0]), 'positive')\n",
    "    )\n",
    "\n",
    "    # Single count operation\n",
    "    total_reviews = df_sentiment.count()\n",
    "    print(f\"\\n‚úÖ Sample dataset created: {total_reviews:,} reviews\")\n",
    "\n",
    "    # Get distribution (single pass)\n",
    "    print(\"\\nüéØ Sentiment distribution:\")\n",
    "    sentiment_counts = df_sentiment.groupBy('sentiment').count().collect()\n",
    "    for row in sentiment_counts:\n",
    "        percentage = (row['count'] / total_reviews) * 100\n",
    "        print(f\"   {row['sentiment']}: {row['count']:,} ({percentage:.1f}%)\")\n",
    "\n",
    "    # Show one sample per sentiment (lightweight)\n",
    "    print(\"\\nüìå Sample reviews:\")\n",
    "    for sent in ['negative', 'neutral', 'positive']:\n",
    "        sample = df_sentiment.filter(F.col('sentiment') == sent).select('text', 'stars').first()\n",
    "        if sample:\n",
    "            print(f\"\\n{sent.upper()} ({sample['stars']} stars):\")\n",
    "            print(f\"   {sample['text'][:150]}...\")\n",
    "\n",
    "    df_sentiment.write.parquet(parquet_path, mode=\"overwrite\")\n",
    "    print(f\"\\nüíæ Parquet saved: {parquet_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+\n",
      "|                text|sentiment|\n",
      "+--------------------+---------+\n",
      "|Not sure who dump...| negative|\n",
      "|I unfortunately h...| negative|\n",
      "|I selected Extrem...|  neutral|\n",
      "|Here is pretty Zo...| positive|\n",
      "|Shaun and the sta...| positive|\n",
      "|Disclaimer. This ...| positive|\n",
      "|After hearing abo...| positive|\n",
      "|Called Blaisdell'...| positive|\n",
      "|Terrible\\nDirty\\n...| negative|\n",
      "|Ya'll tripping fo...| positive|\n",
      "+--------------------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#@title Select relevant features and drop duplicates\n",
    "\n",
    "df_filtered = df_sentiment.select(\"text\", \"sentiment\") \\\n",
    "                .dropDuplicates()\n",
    "\n",
    "df_filtered.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üßπ Text cleaned:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:======================>                                    (3 + 5) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------------------------+--------------------------------------------------------------------------------+---------+\n",
      "|                                                                            text|                                                                      text_clean|sentiment|\n",
      "+--------------------------------------------------------------------------------+--------------------------------------------------------------------------------+---------+\n",
      "|Not sure who dumps out hot coffee 45 minutes prior to closing their doors. Al...|not sure who dumps out hot coffee 45 minutes prior to closing their doors als...| negative|\n",
      "|I unfortunately have had this happen twice now but last week I ordered a part...|i unfortunately have had this happen twice now but last week i ordered a part...| negative|\n",
      "|I selected Extreme Maids to do a move in/deep cleaning. Unfortunately, the mo...|i selected extreme maids to do a move indeep cleaning unfortunately the mover...|  neutral|\n",
      "+--------------------------------------------------------------------------------+--------------------------------------------------------------------------------+---------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#@title Clean text\n",
    "\n",
    "# Add text length\n",
    "df_clean = df_filtered.withColumn('text_length', F.length(F.col('text')))\n",
    "\n",
    "# Add word count\n",
    "df_clean = df_clean.withColumn('word_count', \n",
    "    F.size(F.split(F.col('text'), ' ')))\n",
    "\n",
    "# Clean text: lowercase, remove special characters\n",
    "df_clean = df_clean.withColumn('text_clean',\n",
    "    F.lower(F.regexp_replace(F.col('text'), '[^a-zA-Z0-9\\\\s]', ''))\n",
    ")\n",
    "\n",
    "print(\"\\nüßπ Text cleaned:\")\n",
    "df_clean.select('text', 'text_clean', 'sentiment').show(3, truncate=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìù Sample tokenized text:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------------------------+--------------------------------------------------------------------------------+\n",
      "|                                                                      text_clean|                                                                 tokens_filtered|\n",
      "+--------------------------------------------------------------------------------+--------------------------------------------------------------------------------+\n",
      "|not sure who dumps out hot coffee 45 minutes prior to closing their doors als...|[sure, dumps, hot, coffee, 45, minutes, prior, closing, doors, also, greeted,...|\n",
      "|i unfortunately have had this happen twice now but last week i ordered a part...|[unfortunately, happen, twice, last, week, ordered, part, online, store, show...|\n",
      "|i selected extreme maids to do a move indeep cleaning unfortunately the mover...|[selected, extreme, maids, move, indeep, cleaning, unfortunately, movers, arr...|\n",
      "|here is pretty zoey thank you so much spot on grooming for your excellent ski...|[pretty, zoey, thank, much, spot, grooming, excellent, skills, quality, care,...|\n",
      "|shaun and the staff here are great i was driving home and got a flat they got...|[shaun, staff, great, driving, home, got, flat, got, got, back, road, closed,...|\n",
      "+--------------------------------------------------------------------------------+--------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 13:===================================================>     (9 + 1) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üíæ Parquet saved: ../data/clean/yelp_reviews_tokenized.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#@title Tokenize text\n",
    "\n",
    "# Parquet path\n",
    "parquet_path = \"../data/clean/yelp_reviews_tokenized.parquet\"\n",
    "\n",
    "# Tokenize text\n",
    "tokenizer = Tokenizer(inputCol=\"text_clean\", outputCol=\"tokens\")\n",
    "df_tokenized = tokenizer.transform(df_clean)\n",
    "\n",
    "# Remove stop words\n",
    "remover = StopWordsRemover(inputCol=\"tokens\", outputCol=\"tokens_filtered\")\n",
    "df_tokenized = remover.transform(df_tokenized)\n",
    "\n",
    "print(\"\\nüìù Sample tokenized text:\")\n",
    "df_tokenized.select('text_clean', 'tokens_filtered').show(5, truncate=80)\n",
    "\n",
    "if not os.path.exists(parquet_path):\n",
    "    df_tokenized.write.parquet(parquet_path, mode=\"overwrite\")\n",
    "    print(f\"\\nüíæ Parquet saved: {parquet_path}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "pYXI8VZp5_If"
   ],
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
