{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bgg1FRTSW3dc"
   },
   "source": [
    "# ETL\n",
    "\n",
    "---\n",
    "\n",
    "**Author:** Diego Antonio Garc√≠a Padilla\n",
    "\n",
    "**Date:** Oct 29, 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q7NJ0YPeYDGP"
   },
   "source": [
    "## Enviroment setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 359
    },
    "id": "W4fFJZEDYJQe",
    "outputId": "0ffe40a3-bb98-4480-b921-b4df866ba5ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ENVIRONMENT CHECK ===\n",
      "Python: 3.10.12\n",
      "JAVA_HOME: /usr/lib/jvm/java-8-openjdk-arm64/jre\n",
      "SPARK_HOME: /opt/spark\n",
      "Driver Memory: 12g\n",
      "Executor Memory: 8g\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "#@title Setup & Environment Verification\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "print(\"=== ENVIRONMENT CHECK ===\")\n",
    "print(f\"Python: {sys.version.split()[0]}\")\n",
    "print(f\"JAVA_HOME: {os.environ.get('JAVA_HOME')}\")\n",
    "print(f\"SPARK_HOME: {os.environ.get('SPARK_HOME')}\")\n",
    "print(f\"Driver Memory: {os.environ.get('SPARK_DRIVER_MEMORY')}\")\n",
    "print(f\"Executor Memory: {os.environ.get('SPARK_EXECUTOR_MEMORY')}\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 400
    },
    "id": "5bgOQ8cpWmZX",
    "outputId": "53adea74-2017-427c-b585-78f7a992132b"
   },
   "outputs": [],
   "source": [
    "#@title Import Libraries\n",
    "\n",
    "# PySpark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, CountVectorizer\n",
    "\n",
    "# SciKit Learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Financial data\n",
    "import yfinance as yf\n",
    "\n",
    "# Hugging Face\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "# Kaggle\n",
    "import kagglehub\n",
    "\n",
    "# Utilities\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "import requests\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import subprocess\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Ve3NLAzpd2bJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PRE-FLIGHT CHECK ===\n",
      "Java: ‚úÖ Available\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/11/06 17:41:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/11/06 17:41:52 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßπ Cleaned up existing Spark session\n",
      "==================================================\n",
      "‚úÖ Spark session configured with:\n",
      "   - Driver Memory: 12GB\n",
      "   - Executor Memory: 8GB\n",
      "   - Max Result Size: 4GB\n",
      "   - Parallelism: 16 cores\n",
      "   - Shuffle Partitions: 200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/06 17:41:52 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "#@title Start Spark session\n",
    "\n",
    "print(\"=== PRE-FLIGHT CHECK ===\")\n",
    "\n",
    "# Verify Java is available\n",
    "try:\n",
    "    java_version = subprocess.check_output(['java', '-version'], stderr=subprocess.STDOUT)\n",
    "    print(\"Java: ‚úÖ Available\")\n",
    "except Exception as e:\n",
    "    print(f\"Java: ‚ùå Not available - {e}\")\n",
    "\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# üî• STOP any existing Spark sessions first\n",
    "try:\n",
    "    SparkContext.getOrCreate().stop()\n",
    "    print(\"üßπ Cleaned up existing Spark session\")\n",
    "except:\n",
    "    print(\"üÜï No existing session to clean\")\n",
    "\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create fresh Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Yelp_Sentiment_Analysis\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\", \"12g\") \\\n",
    "    .config(\"spark.executor.memory\", \"8g\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"4g\") \\\n",
    "    .config(\"spark.memory.fraction\", \"0.8\") \\\n",
    "    .config(\"spark.memory.storageFraction\", \"0.3\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n",
    "    .config(\"spark.default.parallelism\", \"16\") \\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "    .config(\"spark.kryoserializer.buffer.max\", \"512m\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "print(\"‚úÖ Spark session configured with:\")\n",
    "print(f\"   - Driver Memory: 12GB\")\n",
    "print(f\"   - Executor Memory: 8GB\")\n",
    "print(f\"   - Max Result Size: 4GB\")\n",
    "print(f\"   - Parallelism: 16 cores\")\n",
    "print(f\"   - Shuffle Partitions: 200\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QTdrxZmA4tbV"
   },
   "source": [
    "## Download data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Yelp Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: /root/.cache/kagglehub/datasets/yelp-dataset/yelp-dataset/versions/4\n",
      "üìÅ Content of: /root/.cache/kagglehub/datasets/yelp-dataset/yelp-dataset/versions/4\n",
      "\n",
      "  üìÑ Dataset_User_Agreement.pdf\n",
      "     Size: 0.08 MB\n",
      "  üìÑ yelp_academic_dataset_business.json\n",
      "     Size: 113.36 MB\n",
      "  üìÑ yelp_academic_dataset_checkin.json\n",
      "     Size: 273.67 MB\n",
      "  üìÑ yelp_academic_dataset_review.json\n",
      "     Size: 5094.40 MB\n",
      "  üìÑ yelp_academic_dataset_tip.json\n",
      "     Size: 172.24 MB\n",
      "  üìÑ yelp_academic_dataset_user.json\n",
      "     Size: 3207.52 MB\n",
      "\n",
      "üíæ Total size: 8.65 GB (8861.26 MB)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9291705417"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#@title Download Yelp reviews dataset\n",
    "\n",
    "yelp_path = kagglehub.dataset_download(\"yelp-dataset/yelp-dataset\")\n",
    "\n",
    "print(\"Path to dataset files:\", yelp_path)\n",
    "\n",
    "def explore_dataset(path):\n",
    "    print(f\"üìÅ Content of: {path}\\n\")\n",
    "    total_size = 0\n",
    "    \n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            size = os.path.getsize(file_path)\n",
    "            total_size += size\n",
    "            size_mb = size / (1024 * 1024)\n",
    "            print(f\"  üìÑ {file}\")\n",
    "            print(f\"     Size: {size_mb:.2f} MB\")\n",
    "    \n",
    "    total_gb = total_size / (1024 * 1024 * 1024)\n",
    "    print(f\"\\nüíæ Total size: {total_gb:.2f} GB ({total_size / (1024 * 1024):.2f} MB)\")\n",
    "    \n",
    "    return total_size\n",
    "\n",
    "explore_dataset(yelp_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Explotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã Schema of Yelp Reviews:\n",
      "root\n",
      " |-- business_id: string (nullable = true)\n",
      " |-- cool: long (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- funny: long (nullable = true)\n",
      " |-- review_id: string (nullable = true)\n",
      " |-- stars: double (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- useful: long (nullable = true)\n",
      " |-- user_id: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Total reviews: 6,990,280\n",
      "\n",
      "üîç Sample reviews:\n",
      "+----------------------+----+-------------------+-----+----------------------+-----+--------------------------------------------------+------+----------------------+\n",
      "|           business_id|cool|               date|funny|             review_id|stars|                                              text|useful|               user_id|\n",
      "+----------------------+----+-------------------+-----+----------------------+-----+--------------------------------------------------+------+----------------------+\n",
      "|XQfwVwDr-v0ZS3_CbbE5Xw|   0|2018-07-07 22:09:11|    0|KU_O5udG6zpxOg-VcAEodg|  3.0|If you decide to eat here, just be aware it is ...|     0|mh_-eMZ6K5RLWhZyISBhwA|\n",
      "|7ATYjTIgM3jUlt4UM3IypQ|   1|2012-01-03 15:28:18|    0|BiTunyQ73aT9WBnpR9DZGw|  5.0|I've taken a lot of spin classes over the years...|     1|OyoGAe7OKpv6SyGZT5g77Q|\n",
      "|YjUWPpI6HXG530lwP-fb2A|   0|2014-02-05 20:30:30|    0|saUsX_uimxRlCVr67Z4Jig|  3.0|Family diner. Had the buffet. Eclectic assortme...|     0|8g_iMtfSiwikVnbP2etR0A|\n",
      "|kxX2SOes4o-D3ZQBkiMRfA|   1|2015-01-04 00:01:03|    0|AqPFMleE6RsU23_auESxiA|  5.0|Wow!  Yummy, different,  delicious.   Our favor...|     1|_7bHUi9Uuf5__HHc_Q8guQ|\n",
      "|e4Vwtrqf-wpJfwesgvdgxQ|   1|2017-01-14 20:54:15|    0|Sx8TMOWLNuJBWer-0pcmoA|  4.0|Cute interior and owner (?) gave us tour of upc...|     1|bcjbaE6dDog4jkNY91ncLQ|\n",
      "+----------------------+----+-------------------+-----+----------------------+-----+--------------------------------------------------+------+----------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "‚≠ê Stars distribution:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+\n",
      "|stars|  count|\n",
      "+-----+-------+\n",
      "|  1.0|1069561|\n",
      "|  2.0| 544240|\n",
      "|  3.0| 691934|\n",
      "|  4.0|1452918|\n",
      "|  5.0|3231627|\n",
      "+-----+-------+\n",
      "\n",
      "\n",
      "üìù Text statistics:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+----------+----------+\n",
      "|       avg_length|min_length|max_length|\n",
      "+-----------------+----------+----------+\n",
      "|567.7644364746477|         1|      5000|\n",
      "+-----------------+----------+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 11:===================================================>    (37 + 3) / 40]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üíæ Parquet saved: ../data/raw/yelp_reviews_raw.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#@title Load Yelp Reviews as Spark dataset\n",
    "\n",
    "# Parquet path\n",
    "parquet_path = \"../data/raw/yelp_reviews_raw.parquet\"\n",
    "\n",
    "if os.path.exists(parquet_path):\n",
    "    print(f\"‚úÖ Already exists: {parquet_path}\")\n",
    "    df_reviews = spark.read.parquet(parquet_path)\n",
    "    print(f\"üîÑ Loaded \\n\")\n",
    "else:\n",
    "    # Load the dataset\n",
    "    reviews_file = os.path.join(yelp_path, \"yelp_academic_dataset_review.json\")\n",
    "\n",
    "    # Read JSON file with Spark\n",
    "    df_reviews = spark.read.json(reviews_file)\n",
    "\n",
    "    # Show schema to understand structure\n",
    "    print(\"üìã Schema of Yelp Reviews:\")\n",
    "    df_reviews.printSchema()\n",
    "\n",
    "    # Basic statistics\n",
    "    print(f\"\\nüìä Total reviews: {df_reviews.count():,}\")\n",
    "\n",
    "    # Show sample data\n",
    "    print(\"\\nüîç Sample reviews:\")\n",
    "    df_reviews.show(5, truncate=50)\n",
    "\n",
    "    # Check stars distribution\n",
    "    print(\"\\n‚≠ê Stars distribution:\")\n",
    "    df_reviews.groupBy('stars').count().orderBy('stars').show()\n",
    "\n",
    "    # Check text lengths\n",
    "\n",
    "    print(\"\\nüìù Text statistics:\")\n",
    "    df_reviews.select(\n",
    "        F.avg(F.length(F.col('text'))).alias('avg_length'),\n",
    "        F.min(F.length(F.col('text'))).alias('min_length'),\n",
    "        F.max(F.length(F.col('text'))).alias('max_length')\n",
    "    ).show()\n",
    "\n",
    "    df_reviews.write.parquet(parquet_path, mode=\"overwrite\")\n",
    "    print(f\"\\nüíæ Parquet saved: {parquet_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Sample dataset created: 1,397,347 reviews\n",
      "\n",
      "üéØ Sentiment distribution:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   positive: 936,359 (67.0%)\n",
      "   neutral: 138,515 (9.9%)\n",
      "   negative: 322,473 (23.1%)\n",
      "\n",
      "üìå Sample reviews:\n",
      "\n",
      "NEGATIVE (2.0 stars):\n",
      "   We started with the artichoke dip, big mistake. It was cold and the chips were stale. Mentioned to our waitress, but nothing came of it.  We ordered a...\n",
      "\n",
      "NEUTRAL (3.0 stars):\n",
      "   Honestly the food doesn't knock my socks off but other people seem to love this place. I go because my husband likes it as for me I'd rather go to a d...\n",
      "\n",
      "POSITIVE (5.0 stars):\n",
      "   Amazingly amazing wings and homemade bleu cheese. Had the ribeye: tender, perfectly prepared, delicious. Nice selection of craft beers. Would DEFINITE...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 21:==================================================>     (36 + 4) / 40]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üíæ Parquet saved: ../data/filtered/yelp_reviews_sentiment.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#@title Sample dataset\n",
    "\n",
    "# Parquet path\n",
    "parquet_path = \"../data/filtered/yelp_reviews_sentiment.parquet\"\n",
    "\n",
    "if os.path.exists(parquet_path):\n",
    "    print(f\"‚úÖ Already exists: {parquet_path}\")\n",
    "    df_sentiment = spark.read.parquet(parquet_path)\n",
    "    print(f\"üîÑ Loaded\")\n",
    "else:\n",
    "    #  Sample directly from the original dataframe with stars\n",
    "    # 10% sample = ~700K reviews (still >1GB when processed with text)\n",
    "    df_sample = df_reviews.sample(fraction=0.20, seed=42)\n",
    "\n",
    "    # Create sentiment column\n",
    "    df_sentiment = df_sample.select(\n",
    "        F.col('review_id'),\n",
    "        F.col('text'),\n",
    "        F.col('stars'),\n",
    "        F.col('useful'),\n",
    "        F.col('date')\n",
    "    ).withColumn('sentiment',\n",
    "        F.when(F.col('stars').isin([1.0, 2.0]), 'negative')\n",
    "        .when(F.col('stars') == 3.0, 'neutral')\n",
    "        .when(F.col('stars').isin([4.0, 5.0]), 'positive')\n",
    "    )\n",
    "\n",
    "    # Single count operation\n",
    "    total_reviews = df_sentiment.count()\n",
    "    print(f\"\\n‚úÖ Sample dataset created: {total_reviews:,} reviews\")\n",
    "\n",
    "    # Get distribution (single pass)\n",
    "    print(\"\\nüéØ Sentiment distribution:\")\n",
    "    sentiment_counts = df_sentiment.groupBy('sentiment').count().collect()\n",
    "    for row in sentiment_counts:\n",
    "        percentage = (row['count'] / total_reviews) * 100\n",
    "        print(f\"   {row['sentiment']}: {row['count']:,} ({percentage:.1f}%)\")\n",
    "\n",
    "    # Show one sample per sentiment (lightweight)\n",
    "    print(\"\\nüìå Sample reviews:\")\n",
    "    for sent in ['negative', 'neutral', 'positive']:\n",
    "        sample = df_sentiment.filter(F.col('sentiment') == sent).select('text', 'stars').first()\n",
    "        if sample:\n",
    "            print(f\"\\n{sent.upper()} ({sample['stars']} stars):\")\n",
    "            print(f\"   {sample['text'][:150]}...\")\n",
    "\n",
    "    df_sentiment.write.parquet(parquet_path, mode=\"overwrite\")\n",
    "    print(f\"\\nüíæ Parquet saved: {parquet_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 22:==================================================>     (36 + 4) / 40]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+\n",
      "|                text|sentiment|\n",
      "+--------------------+---------+\n",
      "|The pasta was coo...| positive|\n",
      "|The food is excel...| negative|\n",
      "|I have only ever ...| negative|\n",
      "|Although the term...| positive|\n",
      "|1st time here, my...| positive|\n",
      "|Cleanliness is an...| negative|\n",
      "|Some cool stuff.....| positive|\n",
      "|I don't know why ...|  neutral|\n",
      "|I love pho and th...| positive|\n",
      "|My husband and I ...|  neutral|\n",
      "+--------------------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#@title Select relevant features and drop duplicates\n",
    "\n",
    "df_filtered = df_sentiment.select(\"text\", \"sentiment\") \\\n",
    "                .dropDuplicates()\n",
    "\n",
    "df_filtered.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîÑ Balancing dataset: 1,396,694 reviews\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 41:==============> (15 + 2) / 17][Stage 43:>                (0 + 4) / 17]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Balanced dataset: 415,431 reviews\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#@title Balance dataset\n",
    "\n",
    "sentiment_counts = df_filtered.groupBy('sentiment').count().collect()\n",
    "min_count = min([row['count'] for row in sentiment_counts]) \n",
    "\n",
    "print(f\"\\nüîÑ Balancing dataset: {df_filtered.count():,} reviews\")\n",
    "\n",
    "df_negative = df_filtered.filter(F.col('sentiment') == 'negative').limit(min_count)\n",
    "df_neutral = df_filtered.filter(F.col('sentiment') == 'neutral').limit(min_count)\n",
    "df_positive = df_filtered.filter(F.col('sentiment') == 'positive').limit(min_count)\n",
    "df_balanced = df_negative.union(df_neutral).union(df_positive)\n",
    "\n",
    "print(f\"\\n‚úÖ Balanced dataset: {df_balanced.count():,} reviews\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üßπ Text cleaned:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 63:==================================================>     (36 + 4) / 40]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------------------------+--------------------------------------------------------------------------------+---------+\n",
      "|                                                                            text|                                                                      text_clean|sentiment|\n",
      "+--------------------------------------------------------------------------------+--------------------------------------------------------------------------------+---------+\n",
      "|The food is excellent. The customer service and quantity over time has contin...|the food is excellent the customer service and quantity over time has continu...| negative|\n",
      "|I have only ever gotten take-out at the Bridgeport Rib House, so I can only c...|i have only ever gotten takeout at the bridgeport rib house so i can only com...| negative|\n",
      "|Cleanliness is an issue here.  It's a great spot for a quick toenail trim but...|cleanliness is an issue here  its a great spot for a quick toenail trim but i...| negative|\n",
      "+--------------------------------------------------------------------------------+--------------------------------------------------------------------------------+---------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#@title Clean text\n",
    "\n",
    "# Add text length\n",
    "df_clean = df_balanced.withColumn('text_length', F.length(F.col('text')))\n",
    "\n",
    "# Add word count\n",
    "df_clean = df_clean.withColumn('word_count', \n",
    "    F.size(F.split(F.col('text'), ' ')))\n",
    "\n",
    "# Clean text: lowercase, remove special characters\n",
    "df_clean = df_clean.withColumn('text_clean',\n",
    "    F.lower(F.regexp_replace(F.col('text'), '[^a-zA-Z0-9\\\\s]', ''))\n",
    ")\n",
    "\n",
    "print(\"\\nüßπ Text cleaned:\")\n",
    "df_clean.select('text', 'text_clean', 'sentiment').show(3, truncate=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìù Sample tokenized text:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------------------------+--------------------------------------------------------------------------------+\n",
      "|                                                                      text_clean|                                                                 tokens_filtered|\n",
      "+--------------------------------------------------------------------------------+--------------------------------------------------------------------------------+\n",
      "|the food is excellent the customer service and quantity over time has continu...|[food, excellent, customer, service, quantity, time, continued, go, ive, goin...|\n",
      "|i have only ever gotten takeout at the bridgeport rib house so i can only com...|[ever, gotten, takeout, bridgeport, rib, house, comment, , numerous, visits, ...|\n",
      "|cleanliness is an issue here  its a great spot for a quick toenail trim but i...|[cleanliness, issue, , great, spot, quick, toenail, trim, smells, like, dog, ...|\n",
      "|i really wanted to love this place since quite a few of my friends just visit...|[really, wanted, love, place, since, quite, friends, visited, past, week, las...|\n",
      "|we went here for breakfast and will never return this restaurant is filthy fl...|[went, breakfast, never, return, restaurant, filthy, floors, walls, windows, ...|\n",
      "+--------------------------------------------------------------------------------+--------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 90:======================================>                   (2 + 1) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üíæ Parquet saved: ../data/clean/yelp_reviews_tokenized.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#@title Tokenize text\n",
    "\n",
    "# Parquet path\n",
    "parquet_path = \"../data/clean/yelp_reviews_tokenized.parquet\"\n",
    "\n",
    "# Tokenize text\n",
    "tokenizer = Tokenizer(inputCol=\"text_clean\", outputCol=\"tokens\")\n",
    "df_tokenized = tokenizer.transform(df_clean)\n",
    "\n",
    "# Remove stop words\n",
    "remover = StopWordsRemover(inputCol=\"tokens\", outputCol=\"tokens_filtered\")\n",
    "df_tokenized = remover.transform(df_tokenized)\n",
    "\n",
    "print(\"\\nüìù Sample tokenized text:\")\n",
    "df_tokenized.select('text_clean', 'tokens_filtered').show(5, truncate=80)\n",
    "\n",
    "if not os.path.exists(parquet_path):\n",
    "    df_tokenized.write.parquet(parquet_path, mode=\"overwrite\")\n",
    "    print(f\"\\nüíæ Parquet saved: {parquet_path}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "pYXI8VZp5_If"
   ],
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
